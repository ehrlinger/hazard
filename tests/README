To illustrate the use of PROC HAZARD and PROC HAZPRED a number of limited data
sets, of a variety of types, are included to illustrate both the procedures and
to provide a new user with practice.  When you unbundle the hazard package, the
examples will be stored in directory $hazard/examples, where $hazard is a Unix
environmental variable that points to the location on your system of the hazard
programs.

The first example is a short (n=310) data set with a few variables of the many
studied is included as '$hazard/examples/data/avc'.  (The lesion is a form of
congenital heart disease known as atrioventricular septal defects, or
atrioventricular (AV) canal.  The entire spectrum of lesions is included from
partial through intermediate to complete forms, in that with this many cases,
the data looked indeed like a spectrum, not 3 isolated forms.) The followup is
such that at present, I find only two hazard phases:  the early phase after the
operation, and a constant hazard phase.  As time goes on, undoutedly the hazard
will again rise, if for no other reason old age.  Duration of followup time, as
well as the actual distribution of events, will dictate how many phases of
hazard are found (NOTE:  we use "phases" as a more understandable term to
clinicians than "mixture distribution components").

Included in '$hazard/examples/macros' are also a number of MACROS that we have
found useful.

     hazplot -- a macro for checking the goodness of fit of the parametric
                model to non-parametric estimates.  Emphasis is placed on the
                empirical cumulative hazard function.  But we have also used the
                theorem of conservation of events to depict observed vs.
                expected deaths.  This curve that ideally is a straight line
                along the line of identity can give clues as to how adequate the
                shape of the estimates are.

     kaplan  -- a macro that simple yields the Kaplan-Meier product limit
                estimates.  We like it better than the procedure in SAS only
                because it does not use the unnecessary Greenwood approximations
                for variance estimation.

     logit   -- a macro that can be used to check out the transformatons that
                may be needed of continuous and ordinal variables.  It is useful
                particularly for the early phase of risk.  It uses a decile
                approach to stratifying the cases (equal number per group).  If
                empirical stratification is desired, use 'logitgr' and set GROUP
                equal to your stratification variable.

Included in '$hazard/examples' are then some actual HAZARD and HAZPRED jobs.

     hz.death.AVC --  This job concentrates on determining the shaping parameter
                estimates for the overall (underlying) distribution of event.
                It is the major exercise that is different from Cox.  It can
                be time-consuming at first, and has all the frustrations of
                nonlinear estimation.  For me, after about 3000 of these
                exercises, the time varies from about 5 minutes to an hour.
                I use the non-parametric cumulative hazard function to
                provide all the clues.  I start with simple models and
                work up (because it is easy to get into an overdetermined
                state).  The final models are rarely complex.  For example, an
                n=6000 coronary data set with 20 years followup will have all
                three phases present, with the hardest to fit late phase
                probably being a simple Weibul (late hazard=MUL*t**power), a
                constant hazard phase, and an early phase with M or NU likely
                simplifying to a constant (1 or 0).  You need not bother with
                DELTA.  But do note that sometimes the program will terminate
                abnomally, but with M or NU driven to near zero (E3 or E4 large
                negative numbers).  THIS IS A CLUE that you want to set NU or M
                to zero (M=0 FIXM, for example).  So when you get abnomal
                terminations, be sure that it is for something other than the
                computer trying to get close to minus infinity.  There are 3
                different optimization algorithms available, and we haven't
                found that we can do without all of them.  In some problems,
                STEEPEST is really needed, in others QUASI works fine (sometimes
                in conjunction with STEEPEST), as does the Newton method.
                Sometimes the early phase will have some huge exponents--but
                there will be one of its 3 branches that will tame these
                exponents.  Read the documentation--I know it is wordy.  Don't
                become frustrated, however.  We have had a number of users with
                initial start-up problems who have simply shipped us a data set
                of events and intervals and a description of their problems, and
                we have been able to help them get a good start.  This effort is
                not terribly time- consuming for us, so do not hesitate in
                asking assistance.

     hm.death.AVC --  This job is an example of a multivariable analysis.  As you
                see, the shaping parameters should be FIXED during the process
                of obtaining the risk factors.  The actual process of doing this
                is no more difficult than a logistic or Cox modeling effort.  We
                have tried, in fact, to make it a bit simpler by allowing you to
                a) exclude variables with an /E option but still look at their
                q-statistics for entry (equivalent to one Newton step), b)
                select variables with associated /S, and c) include variables
                with the /E option.  These are all directly associated with the
                variables, so you do not have to move anything around--in
                particular you can keep your variables ogranized in their
                medically meaningful, and usually highly correlated, groups.  A
                stepwise procedure is provided, and is useful for going a few
                steps (I rarely allow it to go more than 3-5 steps).  But we
                don't recommend automatic model building in absence of knowledge
                of the quality of each variable and the covariance structure.
                The only intellectual hurdle you must overcome is looking
                simultaneously at multiple hazard phases.  However, this "buys"
                relaxation of proportional hazards assumptions, and takes into
                account (especially in surgical series) the well established
                observation that some risk factors are strong early after
                operation, and much less so later.  The job also shows that as a
                final run, we would then suggest unfixing the shaping
                parameters, specifying all the estimates and making a final run.
                In theory, the shaping parameter values must change--we have
                been surprised by how little they change (that is, the average
                curve is not the same as setting all risk factors to zero).  You
                may wish also to reexplore the early phase shaping parameters in
                the rare event that THALF gets much larger in this final step.

     hp.death.AVC.hm1 and hp.death.hm2 -- These are examples of exploring the
                strengths of risk factors either at a slice across time
                (hm.death.hm1) or across time (hm.death.hm1).  The advantage of
                an all parametric model is that all you have to do is solve an
                equation!

     hs.death.AVC.hm1 -- This is an example of the important process of internally
                validating the analysis.  (Or of invalidating it, as some would
                say).  What we illustrate is two forms of validation, if you
                will allow a loose use of the word.  First, by the conservation
                of event theorem, one can calculate for each patient's
                individual time of followup and risk factors a value at that
                time of his cumulative hazard.  The sum of all these will
                exactly add to the number of total deaths in the data set.  You
                can now stratify the patients according, for example, to some
                variable that you think is interesting but has not come into the
                model, and compare (eg, chi square goodness of fit test)
                observed deaths in each strata with those predicted (the sum of
                all predicted cumulative hazards in the stratum).  One can also
                calculate a time-related curve for each patient.  Then by the
                theory of mixture distributions, you can form an average
                survival curve by summing the curves and dividing by the n of
                each stratum.  These can be superimposed on the life table
                estimates for each stratum and compared visually.  Departures
                from a really good fit suggest inadequate modeling
                (transformation of variables, not taking some variable into
                account, etc).  These kinds of activities are not common in the
                literature.  And the latter, in particular, takes a fair amount
                of computer time and resources.  However, a serious modeling
                effort seems to us incomplete without some attempt to see how
                badly we've performed!

For shear practice of fitting a variety of hazard shapes with up to three
phases, you may now wish to explore the coronary artery bypass grafting data
set.  It contains the event death, return of angina, and reintervention.  To
get you started, hz.deadp.KUL is an annotated setup for the event death.  You
will want to start off with some actuarial estimates, as you did for the data
set above, and work your way through all these events.

For practice at both fitting interesting hazard shapes, controversy as to what
events should be "bundled" as a single event, and then exercising of the
procedure's variable selection possibilities, a limited valve replacement data
set ($hazard/examples/data/valves) is provided, along with a sample
multivariable setup program hm.deadp.VALVES.

For a complete change of pace, another data set is included that has within it a
repeated event (thromboembolism) and an evaluation of the seriousnes of the
event.  In hz.te123 we look at two ways of analyzing such data:  straight
repeated event methodology using longitudinal segmentation of the observation as
described in the documentation and left censoring, and a modulated renewal
process.  When the latter is valid, it makes for a much more interpretable
analysis, in my opinion.  In hz.tm123, the event is analyzed as a weighted
outcome variable, using the WEIGHT statement.

Since an important aspect of the model is exercising the resulting equations,
we have included a number of plots (hp. jobs) as well as a number of validation
files (the hs jobs).  These are internally commented.

Another use of predictions is for comparing alternative treatments from analyses
of different data sets.  One example of this is given in hp.death.COMPARISON,
an example of comparison of PTCA, CABG and medical treatment.
